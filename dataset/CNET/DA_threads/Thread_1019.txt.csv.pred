1.1.1	hi guys , Straight to the point , i have a very high number of files which are over 2,000,000,000 Nos , the volume of each file is between 1 KB and 30 KB , and because of the enormous count of files , copying them into a NAS ( Network Access Server which contains 8 HDDs that are in 10 RAID-resulting in a 6GB Drive ) is a complicated task .	St
1.1.2	copying through windows ordinary copy paste usually takes years ( ! ! ! )	St
1.1.3	to even calculate the task progress , and i also have tried using following softwares : UltraCopy , TeraCopy , above softwares usually fail in the first initial minutes , the following software-which is a great one for Copy Tasks- does not hang but again takes a very long time : RichCopy also i tried using Norton Ghost , which takes a image of the source drive in a short time , but again copying from the image to the destination drive is very very slow .	St
1.1.4	I 'm looking for a solution which can copy the files from source HDD without worrying about dealing with every and each one of files , but only copying segments of HDD no matter how many files are located in them , perhaps this solution will result in a very faster copy .	St
1.1.5	I would appreciate if you offer me any professional solution for this issue .	Polite
1.1.6	Thank you !	St
1.2.1	You ca n't just copy a segment of the HDD .	St
1.2.2	There 's the FAT and most serious , there 's the directory that has to be modified for each and every file .	St
1.2.3	And 2 billion directory entries is a lot .	St
1.2.4	So I would go for a different solution .	St
1.2.5	Store each file as a record in a database .	St
1.2.6	Databases are optimised for such work .	St
1.2.7	And they support multiple load tasks at the same time .	St
1.2.8	And it would be easy to add metadata to each file , which greatly helps to retrieve the one from the 2 billion that you need .	St
1.2.9	Selecting the right DBMS ( it will be 2 to 60 Terabyte only for the data , if you do n't compress them , with a comparable space for the metadata and the indexes ) certainly is a job for a professional .	St
1.2.10	The same goes for the technical database design , the writing of the programs to load the data and to retrieve them again and for the performance monitoring .	St
1.2.11	And do n't forget the backup ( another X TB ) .	Sug
1.2.12	So this seems a 6-digit project ( somewhere between $ 100.000 and $ 1.000.000 ) .	St
1.2.13	And 6-digit projects are n't done via consumer oriented forums , but by companies .	St
1.2.14	Kees	St
1.3.1	There is another issue and that is the speed loss as you pass some number of entries in a folder .	St
1.3.2	Kees noted why such things move from files to a DBMS .	St
1.3.3	I 'm only adding another reason why it slows down .	St
1.3.4	Once in awhile you will encounter someone that designed a system all file based with millions of files and it works on a local HDD and is a dog over a network .	St
1.3.5	About half the time they call you an idiot because you ca n't solve the issue and the other half already knew they were in trouble .	St
1.3.6	I see you found the usual copy apps so I 'll add RoadKil 's Unstoppable Copier to the list as well as the command line method .	St
1.3.7	Any database admin knows the command line copy functions so I wo n't dive into that .	St
1.3.8	In my opinion , work with the company to help them figure out that 2 million files in a system is not suitable for a network app .	St
1.3.9	Bob	St
