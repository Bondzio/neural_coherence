11425	0	299067	2798845	NR	barkal_l	6/24/08 10:25 PM	1.2143463E12	Is GTX260 price premium worth 30W (idle) savings over 9800+?	AnandTech benchmarks seem to show that the 260 runs about 30W less at idle and that the two are essentially equal under load. Given that the card will spend a vast majority of time at or near idle, does it make sense to spend nearly double the price, once the 9800 price cuts hit (making the GTX+ roughly $230)?
11425	1	299067	2798852	2798845	squirtlewa	6/24/08 10:45 PM	1.2143475E12	what's your metric?	At current California prices for electricity the financial cost for the electrical power difference between the cards would be $37.74 (based on $.1436 per kWh) per year, if you run your PC 24/7. I'm clueless how many tonnes of CO2 that is. Obviously it'd be cooler to run. And sexier cuz it's new. XD
11425	2	299067	2798960	2798845	R. Proffitt	6/25/08 4:56 AM	1.21436976E12	Electricity 10.65 cents / KWh (Mass web site)	But lets take a real bill and find a number to use. Using the total bill divided by the number of KWh I arrive at 15.68 cents per KWh. 1 year = 8,760 hours 30 Watts for 1 year = 262.8 KWh or 41.21 bucks. You decide here. But your usual gamer desktops idles at 200 Watts so let's do that. About 275 a year. Bob
11425	3	299067	2798993	2798845	ramarc	6/25/08 6:44 AM	1.21437624E12	no	you buy a high performance video card based on it's gaming performance, not it's power draw. if you only game at 1680x1050, a 9800gtx will play most every game just as smoothly as a gtx 260. also consider the new hd 4870 which gives comparable performance to a gtx 260 for $100 less.
