Hey all- Thanks for the input on whether or not to go DVI.
I actually bought a Sceptre x20wg (DVI) from a guy through craigslist.org, and he bought it about a month ago from newegg.com.
He had no problems with the cable or monitor.
Within a few hours of plugging it into my comp, the monitor would randomly go black and display a No Signal Detected message.
It would also do this when playing a game, or when Windows would shut down the monitor as a power saving measure.
Other times it would randomly flicker.
Usually I can unplug the cable and plug it back in, and the picture will come back/stabilize.
I have an ATI x700 card, and this is the first time I've used the DVI port.
All drivers for both the monitor and card are updated.
ATI's site says to increase the refresh, so I went from 60 to 75 (75 was the next available option), and that didn't help.
Any ideas?
I'm going to try a VGA cable tomorrow, and I have another DVI cable on the way, but since it worked for the previous owner, I'm a little stumped.
