hard drive manufacturers publish a host of data on their products - i don't mean the manufacturer of the finished product, but the manufacturer of the actual drives themselves.
i have a usb raid drive assembly made by cavalier, for instance, that has two western digital drives inside.
from a &quot;predict the future&quot; perspective, mtbf (mean time between failures) will enable a system designer or integrator to calculate how many drives can fail within a given time period, thereby enabling her to ensure an appropriate number of spares are on hand, but there isn't any way to calculate when it is appropriate to replace a drive.
too many factors are variable - drive orientation, drive temperature, ambient temperature, cooling method, write cycles per hour, i could go on.
most importantly, data stored on any type of drive - magnetical, optical, solid state - is never secure.
especially writable optical media has a high risk of failure, but the fact that other storage media is less at risk means little, other than that they can - and will - all eventually fail.
i back all of my systems up to a small - 1.5 terabytes - raid server array on my home network.
this backs itself up to yet another raid drive assembly, a couple of times a day.
for each of the drives i use, i have a spare, and once a year or so i &quot;back up the backup&quot; and swap the drives out.
while this method is not 100% failsafe, it approaches what we refer to as &quot;five nines&quot; in the industry - a statistical uptime availability of 99.999%.
this may sound a very high number, but 99.999% still translates to a downtime of a little over five hours a year, in an assembly that runs 24/7/365.
online backup services cannot be considered reliable, either, as you have no control over the way the vendor maintains their server park, or over what happens to your data should the server owner go bankrupt.
where data stored with amazon, google or ibm is probably safe, as these companies make server space available in server parks they use to store their own end product on, there are ultimately no guarantees.
the assembly i use was relatively cheap to buy and put together, and takes little time to maintain.
decide how important it is to have access to the data stored - there are guidelines how long tax data must be stored, for instance, and you probably don't want to lose quicken data with mortgage information for the length of the mortgage, assuming you have &quot;gone digital&quot; with this, as i have.
the best you can hope to do is have regularly maintained storage available, to back up the backups, and to ensure sufficient ups (uninterruptible power supply) electricity is available online to power down drive systems in an organized manner when a power failure occurs - a drive that loses power during a write operation has a good chance of becoming corrupted, and there is no statistic that covers this common cause of data loss.
one method to keep an eye on drive performance is regular defragmentation of the data on the drive.
windows vista can do this automatically, even on a daily basis, assuming a system is kept &quot;on&quot; 24/7.
&quot;defragging&quot; will provide an early warning of impending drive failure, because the routine will provide error messages when it detects bad areas on the drive it is defragmenting.
at the first error message, it is time to immediately back up all data, and then replace the affected drive - assuming you have a spare on hand, one that uses the same form factor and interface technology the original equipment did.
